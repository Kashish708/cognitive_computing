{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4utA8DLMi6f",
        "outputId": "fa2ce364-a0c6-4792-ccb7-89b749e135be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'learning': 2, 'language': 2, 'love': 1, 'new': 1, 'technologies': 1, 'artificial': 1, 'intelligence': 1, 'evolving': 1, 'rapidly': 1, 'python': 1, 'favorite': 1, 'programming': 1, 'simplicity': 1, 'power': 1, 'every': 1, 'year': 1, 'see': 1, 'breakthroughs': 1, 'natural': 1, 'processing': 1, 'machine': 1, 'makes': 1, 'computers': 1, 'smarter': 1, 'useful': 1, 'tech': 1, 'world': 1, 'always': 1, 'full': 1, 'surprises': 1, 'innovations': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "text = \"\"\"I love learning about new technologies. Artificial Intelligence is evolving rapidly.\n",
        "Python is my favorite programming language because of its simplicity and power.\n",
        "Every year, we see breakthroughs in natural language processing.\n",
        "Machine learning makes computers smarter and more useful.\n",
        "The tech world is always full of surprises and innovations.\"\"\"\n",
        "\n",
        "\n",
        "text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "\n",
        "freq_dist = Counter(filtered_tokens)\n",
        "print(freq_dist)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "porter_stems = [porter.stem(word) for word in filtered_tokens]\n",
        "lancaster_stems = [lancaster.stem(word) for word in filtered_tokens]\n",
        "\n",
        "\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"Porter Stemmer:\", porter_stems)\n",
        "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
        "print(\"Lemmatized:\", lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW1YdSyLMt93",
        "outputId": "f7ff55ac-1039-4bec-c6bd-aa7469e9aa16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['love', 'learn', 'new', 'technolog', 'artifici', 'intellig', 'evolv', 'rapidli', 'python', 'favorit', 'program', 'languag', 'simplic', 'power', 'everi', 'year', 'see', 'breakthrough', 'natur', 'languag', 'process', 'machin', 'learn', 'make', 'comput', 'smarter', 'use', 'tech', 'world', 'alway', 'full', 'surpris', 'innov']\n",
            "Lancaster Stemmer: ['lov', 'learn', 'new', 'technolog', 'art', 'intellig', 'evolv', 'rapid', 'python', 'favorit', 'program', 'langu', 'simpl', 'pow', 'every', 'year', 'see', 'breakthrough', 'nat', 'langu', 'process', 'machin', 'learn', 'mak', 'comput', 'smart', 'us', 'tech', 'world', 'alway', 'ful', 'surpr', 'innov']\n",
            "Lemmatized: ['love', 'learning', 'new', 'technology', 'artificial', 'intelligence', 'evolving', 'rapidly', 'python', 'favorite', 'programming', 'language', 'simplicity', 'power', 'every', 'year', 'see', 'breakthrough', 'natural', 'language', 'processing', 'machine', 'learning', 'make', 'computer', 'smarter', 'useful', 'tech', 'world', 'always', 'full', 'surprise', 'innovation']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "long_words = [word for word in tokens if len(word) > 5]\n",
        "\n",
        "\n",
        "numbers = re.findall(r'\\d+', text)\n",
        "\n",
        "\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', \"\"\"Original Text Here\"\"\")\n",
        "\n",
        "\n",
        "alpha_words = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "\n",
        "vowel_words = [word for word in tokens if word[0] in 'aeiou']\n",
        "\n",
        "print(\"Long words:\", long_words)\n",
        "print(\"Numbers:\", numbers)\n",
        "print(\"Capitalized words:\", capitalized_words)\n",
        "print(\"Only alphabets:\", alpha_words)\n",
        "print(\"Words starting with vowels:\", vowel_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPnpgDiONLdn",
        "outputId": "2c117b2c-8af6-41ff-d7ec-e0493be53860"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long words: ['learning', 'technologies', 'artificial', 'intelligence', 'evolving', 'rapidly', 'python', 'favorite', 'programming', 'language', 'because', 'simplicity', 'breakthroughs', 'natural', 'language', 'processing', 'machine', 'learning', 'computers', 'smarter', 'useful', 'always', 'surprises', 'innovations']\n",
            "Numbers: []\n",
            "Capitalized words: ['Original', 'Text', 'Here']\n",
            "Only alphabets: ['i', 'love', 'learning', 'about', 'new', 'technologies', 'artificial', 'intelligence', 'is', 'evolving', 'rapidly', 'python', 'is', 'my', 'favorite', 'programming', 'language', 'because', 'of', 'its', 'simplicity', 'and', 'power', 'every', 'year', 'we', 'see', 'breakthroughs', 'in', 'natural', 'language', 'processing', 'machine', 'learning', 'makes', 'computers', 'smarter', 'and', 'more', 'useful', 'the', 'tech', 'world', 'is', 'always', 'full', 'of', 'surprises', 'and', 'innovations']\n",
            "Words starting with vowels: ['i', 'about', 'artificial', 'intelligence', 'is', 'evolving', 'is', 'of', 'its', 'and', 'every', 'in', 'and', 'useful', 'is', 'always', 'of', 'and', 'innovations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def custom_tokenize(text):\n",
        "\n",
        "    text = re.sub(r\"(\\w)(')(\\w)\", r\"\\1\\2\\3\", text)\n",
        "\n",
        "\n",
        "    text = re.sub(r\"(\\w+)-(\\w+)\", r\"\\1-\\2\", text)\n",
        "\n",
        "\n",
        "    tokens = re.findall(r\"\\d+\\.\\d+|\\w+(?:'\\w+)?|[\\w-]+\", text)\n",
        "    return tokens\n",
        "\n",
        "custom_tokens = custom_tokenize(\"\"\"Email me at john@example.com or visit https://my-site.org.\n",
        "My number is 123-456-7890 or +91 9876543210.\"\"\")\n",
        "\n",
        "print(\"Custom Tokens:\", custom_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqgsmuxpNYQ2",
        "outputId": "39cf8520-5eab-4423-9fa6-f5a2dd29b4c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokens: ['Email', 'me', 'at', 'john', 'example', 'com', 'or', 'visit', 'https', 'my', '-site', 'org', 'My', 'number', 'is', '123', '-456-7890', 'or', '91', '9876543210']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Email me at john@example.com or visit https://my-site.org.\n",
        "My number is 123-456-7890 or +91 9876543210.\"\"\"\n",
        "\n",
        "\n",
        "text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
        "\n",
        "\n",
        "text = re.sub(r'http\\S+|www.\\S+', '<URL>', text)\n",
        "\n",
        "\n",
        "text = re.sub(r'(\\+?\\d{1,3}[- ]?)?\\d{3}[- ]?\\d{3}[- ]?\\d{4}', '<PHONE>', text)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRDqQeoKNb96",
        "outputId": "3083922a-47d3-483e-fe09-a963e5b72ce3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email me at <EMAIL> or visit <URL> \n",
            "My number is <PHONE> or <PHONE>.\n"
          ]
        }
      ]
    }
  ]
}